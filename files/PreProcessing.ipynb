{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc807a8-5375-4a47-937d-4d49acbd0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balancing\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to balance the dataset\n",
    "def balance_dataset(original_videos_dir, manipulated_videos_dir, output_dir, target_count=150):\n",
    "    \"\"\"\n",
    "    Creates a balanced dataset by sampling an equal number of videos from\n",
    "    the original and manipulated video directories.\n",
    "\n",
    "    Args:\n",
    "        original_videos_dir (Path): Path to the directory with original videos.\n",
    "        manipulated_videos_dir (Path): Path to the directory with manipulated videos.\n",
    "        output_dir (Path): Path to the directory where the balanced dataset will be saved.\n",
    "        target_count (int): Number of videos to sample for each class.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two tuples containing sampled videos and their corresponding labels.\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    balanced_original_dir = output_dir / \"original\"\n",
    "    balanced_manipulated_dir = output_dir / \"manipulated\"\n",
    "    balanced_original_dir.mkdir(parents=True, exist_ok=True)\n",
    "    balanced_manipulated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Gather video lists\n",
    "    original_videos = list(original_videos_dir.glob(\"*.mp4\"))\n",
    "    manipulated_videos = list(manipulated_videos_dir.glob(\"*.mp4\"))\n",
    "\n",
    "    # Check if enough videos are available\n",
    "    if len(original_videos) < target_count or len(manipulated_videos) < target_count:\n",
    "        raise ValueError(f\"Not enough videos to sample {target_count} videos from one or both classes.\")\n",
    "\n",
    "    # Random sampling\n",
    "    random.seed(42)  # For reproducibility\n",
    "    sampled_original = random.sample(original_videos, target_count)\n",
    "    sampled_manipulated = random.sample(manipulated_videos, target_count)\n",
    "\n",
    "    # Copy sampled videos to output directories\n",
    "    for file in sampled_original:\n",
    "        shutil.copy(file, balanced_original_dir / file.name)\n",
    "    for file in sampled_manipulated:\n",
    "        shutil.copy(file, balanced_manipulated_dir / file.name)\n",
    "\n",
    "    print(f\"Balanced dataset created with {target_count} videos in each class.\")\n",
    "    return (sampled_original, [0] * len(sampled_original)), (sampled_manipulated, [1] * len(sampled_manipulated))\n",
    "\n",
    "\n",
    "# Paths to the datasets\n",
    "original_videos_dir = Path(\"M-DATA/Celeb-real\")  # Adjust the path if needed\n",
    "manipulated_videos_dir = Path(\"M-DATA/Celeb-synthesis\")\n",
    "output_dir = Path(\"./balanced_dataset3\")\n",
    "\n",
    "# Count the number of videos in the original directories\n",
    "num_original_videos = len(list(original_videos_dir.glob(\"*.mp4\")))\n",
    "num_manipulated_videos = len(list(manipulated_videos_dir.glob(\"*.mp4\")))\n",
    "\n",
    "print(f\"Original videos available: {num_original_videos}\")\n",
    "print(f\"Manipulated videos available: {num_manipulated_videos}\")\n",
    "\n",
    "# Create the balanced dataset\n",
    "try:\n",
    "    balanced_samples = balance_dataset(\n",
    "        original_videos_dir, \n",
    "        manipulated_videos_dir, \n",
    "        output_dir, \n",
    "        target_count=500\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d21e2-4bb5-4272-af7c-f676acea5984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86013ca3-b951-4206-9e45-dd197bda18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped and resized images saved in balanced_dataset_cropped_faces\\manipulated.\n"
     ]
    }
   ],
   "source": [
    "#EXTRACTING FRAMES\n",
    "\n",
    "#MAIN WORKING CODE\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "def crop_faces_from_videos(video_dir, output_dir, face_cascade_path, num_images=10, resize_dim=(224, 224)):\n",
    "    \"\"\"\n",
    "    Extracts cropped face images from each video, resizes them to a specified dimension, \n",
    "    and saves them in separate folders for each video.\n",
    "\n",
    "    Args:\n",
    "        video_dir (Path): Path to the directory containing videos.\n",
    "        output_dir (Path): Path to the directory where cropped images will be saved.\n",
    "        face_cascade_path (str): Path to the Haar cascade XML file for face detection.\n",
    "        num_images (int): Number of cropped images to extract per video.\n",
    "        resize_dim (tuple): Target size for resizing the cropped face images (width, height).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load Haar Cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "    if face_cascade.empty():\n",
    "        raise ValueError(\"Error loading Haar cascade XML file.\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process each video in the directory\n",
    "    for video_path in video_dir.glob(\"*.mp4\"):\n",
    "        video_name = video_path.stem\n",
    "        video_output_dir = output_dir / video_name\n",
    "        video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Calculate frame interval\n",
    "        frame_interval = max(total_frames // num_images, 1)  # Avoid division by zero\n",
    "\n",
    "        frame_count = 0\n",
    "        images_saved = 0\n",
    "\n",
    "        while cap.isOpened() and images_saved < num_images:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process every `frame_interval` frame\n",
    "            if frame_count % frame_interval == 0:\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "                for (x, y, w, h) in faces:\n",
    "                    cropped_face = frame[y:y + h, x:x + w]\n",
    "\n",
    "                    # Resize cropped face to the target dimensions\n",
    "                    resized_face = cv2.resize(cropped_face, resize_dim)\n",
    "\n",
    "                    # Save the resized face image\n",
    "                    output_image_path = video_output_dir / f\"frame{images_saved}_face.jpg\"\n",
    "                    cv2.imwrite(str(output_image_path), resized_face)\n",
    "                    images_saved += 1\n",
    "                    break  # Save only one face per frame\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    print(f\"Cropped and resized images saved in {output_dir}.\")\n",
    "\n",
    "# Paths to videos and output directory\n",
    "# original_videos_dir = Path(\"balanced_dataset3/original\")  # Path to original videos\n",
    "manipulated_videos_dir = Path(\"balanced_dataset3/manipulated\")  # Path to manipulated videos\n",
    "output_dir = Path(\"./balanced_dataset_cropped_faces\")  # Output directory for cropped images\n",
    "face_cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# # Process original and manipulated videos\n",
    "# crop_faces_from_videos(original_videos_dir, output_dir / \"original\", face_cascade_path, resize_dim=(224, 224))\n",
    "crop_faces_from_videos(manipulated_videos_dir, output_dir / \"manipulated\", face_cascade_path, resize_dim=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80b512-77f6-45d1-addb-83ec06ec16d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d541e0-c309-4722-b6de-6ed4117f9944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
